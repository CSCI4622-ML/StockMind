{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Normalizes all columns in a pandas DataFrame  using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The normalized DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = [col for col in df.columns]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>WMA</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. adjusted close</th>\n",
       "      <th>6. volume</th>\n",
       "      <th>7. dividend amount</th>\n",
       "      <th>8. split coefficient</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54006</th>\n",
       "      <td>0.828590</td>\n",
       "      <td>0.611121</td>\n",
       "      <td>0.124368</td>\n",
       "      <td>0.161278</td>\n",
       "      <td>0.721455</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.371731</td>\n",
       "      <td>0.908468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912402</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.792715</td>\n",
       "      <td>0.795367</td>\n",
       "      <td>0.785348</td>\n",
       "      <td>0.915660</td>\n",
       "      <td>0.053093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54007</th>\n",
       "      <td>0.831386</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>0.120646</td>\n",
       "      <td>0.147684</td>\n",
       "      <td>0.739753</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.361116</td>\n",
       "      <td>0.912497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915420</td>\n",
       "      <td>0.780175</td>\n",
       "      <td>0.785976</td>\n",
       "      <td>0.794163</td>\n",
       "      <td>0.787510</td>\n",
       "      <td>0.917801</td>\n",
       "      <td>0.047816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54008</th>\n",
       "      <td>0.832392</td>\n",
       "      <td>0.613550</td>\n",
       "      <td>0.109766</td>\n",
       "      <td>0.139020</td>\n",
       "      <td>0.741281</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.358681</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917420</td>\n",
       "      <td>0.777547</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.786126</td>\n",
       "      <td>0.778268</td>\n",
       "      <td>0.908648</td>\n",
       "      <td>0.041491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54009</th>\n",
       "      <td>0.836382</td>\n",
       "      <td>0.618820</td>\n",
       "      <td>0.098034</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.750070</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.349887</td>\n",
       "      <td>0.916590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919478</td>\n",
       "      <td>0.775004</td>\n",
       "      <td>0.777489</td>\n",
       "      <td>0.784578</td>\n",
       "      <td>0.780133</td>\n",
       "      <td>0.910496</td>\n",
       "      <td>0.049474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54010</th>\n",
       "      <td>0.838405</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.096450</td>\n",
       "      <td>0.133156</td>\n",
       "      <td>0.760952</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.345329</td>\n",
       "      <td>0.918329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922155</td>\n",
       "      <td>0.781107</td>\n",
       "      <td>0.788834</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.788697</td>\n",
       "      <td>0.918977</td>\n",
       "      <td>0.044220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  \\\n",
       "54006     0.828590  0.611121  0.124368  0.161278  0.721455        0.35   \n",
       "54007     0.831386  0.613370  0.120646  0.147684  0.739753        0.30   \n",
       "54008     0.832392  0.613550  0.109766  0.139020  0.741281        0.25   \n",
       "54009     0.836382  0.618820  0.098034  0.135600  0.750070        0.20   \n",
       "54010     0.838405  0.621768  0.096450  0.133156  0.760952        0.15   \n",
       "\n",
       "       Aroon Up  AROONOSC       ATR  Real Upper Band  ...       WMA   1. open  \\\n",
       "54006      1.00     0.825  0.371731         0.908468  ...  0.912402  0.789583   \n",
       "54007      0.95     0.825  0.361116         0.912497  ...  0.915420  0.780175   \n",
       "54008      0.90     0.825  0.358681         0.914832  ...  0.917420  0.777547   \n",
       "54009      0.85     0.825  0.349887         0.916590  ...  0.919478  0.775004   \n",
       "54010      0.80     0.825  0.345329         0.918329  ...  0.922155  0.781107   \n",
       "\n",
       "        2. high    3. low  4. close  5. adjusted close  6. volume  \\\n",
       "54006  0.792715  0.795367  0.785348           0.915660   0.053093   \n",
       "54007  0.785976  0.794163  0.787510           0.917801   0.047816   \n",
       "54008  0.778555  0.786126  0.778268           0.908648   0.041491   \n",
       "54009  0.777489  0.784578  0.780133           0.910496   0.049474   \n",
       "54010  0.788834  0.792659  0.788697           0.918977   0.044220   \n",
       "\n",
       "       7. dividend amount  8. split coefficient  company  \n",
       "54006                 0.0                   0.0       10  \n",
       "54007                 0.0                   0.0       10  \n",
       "54008                 0.0                   0.0       10  \n",
       "54009                 0.0                   0.0       10  \n",
       "54010                 0.0                   0.0       10  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_csvs_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder into a single pandas DataFrame also normalizes before combining them.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the concatenated data from all CSV files in the input folder.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension to read all CSV files in the folder into a list of DataFrames.\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # Use a list comprehension to get the filenames of all CSV files in the folder.\n",
    "    filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    processed_dfs = []\n",
    "    i = 0\n",
    "    for df, filename in zip(dfs, filenames):\n",
    "        # Dont need the date column\n",
    "        df = df.drop(['date'], axis=1)\n",
    "        # normalize the dataframes before combining them\n",
    "        df = normalize_dataframe(df)\n",
    "        # for the neural network to understand the company name we need to convert it to a number\n",
    "        df['company'] = i\n",
    "        i += 1\n",
    "        processed_dfs.append(df)\n",
    "    combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "df = combine_csvs_from_folder('market_data/merged_data')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need this for later\n",
    "def find_indices_of_last_company_changes(df):\n",
    "    indices = []\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, 'company'] != df.loc[i - 1, 'company']:\n",
    "            indices.append(i-1)\n",
    "    return indices\n",
    "idxs = find_indices_of_last_company_changes(df)\n",
    "idxs.append(len(df) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should one hot encode the company column\n",
    "# first we need to change it to a string so we can one hot encode it\n",
    "df['company'] = df['company'].astype(str)\n",
    "df = pd.get_dummies(df, columns=['company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>company_10</th>\n",
       "      <th>company_2</th>\n",
       "      <th>company_3</th>\n",
       "      <th>company_4</th>\n",
       "      <th>company_5</th>\n",
       "      <th>company_6</th>\n",
       "      <th>company_7</th>\n",
       "      <th>company_8</th>\n",
       "      <th>company_9</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172135</td>\n",
       "      <td>0.545425</td>\n",
       "      <td>0.154470</td>\n",
       "      <td>0.101243</td>\n",
       "      <td>0.502764</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.169640</td>\n",
       "      <td>0.532056</td>\n",
       "      <td>0.164471</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.502062</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.173826</td>\n",
       "      <td>0.555077</td>\n",
       "      <td>0.164130</td>\n",
       "      <td>0.114041</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.011184</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.177859</td>\n",
       "      <td>0.590061</td>\n",
       "      <td>0.159311</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.501608</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.173893</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.149749</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.501044</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  Aroon Up  \\\n",
       "0     0.172135  0.545425  0.154470  0.101243  0.502764        0.95      0.25   \n",
       "1     0.169640  0.532056  0.164471  0.106557  0.502062        0.90      0.20   \n",
       "2     0.173826  0.555077  0.164130  0.114041  0.502022        0.85      0.15   \n",
       "3     0.177859  0.590061  0.159311  0.123042  0.501608        0.80      0.10   \n",
       "4     0.173893  0.583195  0.149749  0.131813  0.501044        0.75      0.05   \n",
       "\n",
       "   AROONOSC       ATR  Real Upper Band  ...  company_10  company_2  company_3  \\\n",
       "0      0.15  0.010778         0.004883  ...           0          0          0   \n",
       "1      0.15  0.010778         0.004920  ...           0          0          0   \n",
       "2      0.15  0.011184         0.004920  ...           0          0          0   \n",
       "3      0.15  0.011078         0.004897  ...           0          0          0   \n",
       "4      0.15  0.011167         0.004806  ...           0          0          0   \n",
       "\n",
       "   company_4  company_5  company_6  company_7  company_8  company_9  up  \n",
       "0          0          0          0          0          0          0   0  \n",
       "1          0          0          0          0          0          0   0  \n",
       "2          0          0          0          0          0          0   1  \n",
       "3          0          0          0          0          0          0   1  \n",
       "4          0          0          0          0          0          0   0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_up_column(df):\n",
    "    # Create empty 'up' and 'down' columns\n",
    "    df['up'] = 0\n",
    "    \n",
    "    # Loop over the rows (skipping the first row)\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, '4. close'] > df.loc[i-1, '4. close']:\n",
    "            df.loc[i, 'up'] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "df = add_up_column(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1721, 0.5454, 0.1545, 0.1012, 0.5028, 0.9500, 0.2500, 0.1500, 0.0108,\n",
      "        0.0049, 0.0044, 0.0048, 0.6001, 0.3041, 0.3204, 0.0041, 0.3287, 0.0043,\n",
      "        0.0770, 0.1221, 0.4935, 0.5869, 0.4913, 0.8474, 0.0045, 1.0000, 0.0041,\n",
      "        0.4687, 0.4572, 0.5075, 0.5028, 0.4830, 0.5257, 0.0021, 0.0000, 0.4928,\n",
      "        0.0043, 0.0044, 0.4145, 0.0082, 0.4762, 0.3511, 0.2082, 0.1884, 0.0058,\n",
      "        0.7284, 0.5345, 0.5344, 0.3204, 0.0051, 0.0044, 0.1780, 0.2642, 0.3207,\n",
      "        0.1780, 0.2269, 0.0756, 0.0045, 0.0039, 0.0067, 0.0046, 0.7760, 0.3548,\n",
      "        0.1964, 0.0043, 0.1429, 0.1544, 0.1394, 0.1461, 0.0037, 0.0121, 0.0000,\n",
      "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000])\n",
      "torch.Size([54011, 84])\n",
      "torch.Size([54011, 1])\n"
     ]
    }
   ],
   "source": [
    "# neural networks require tensors, so we need to convert our dataframes to tensors\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    inputs_columns = df.columns[df.columns != 'up']\n",
    "    inputs = torch.from_numpy(df.loc[:, inputs_columns].values.astype('float32'))\n",
    "    targets = torch.from_numpy(df.loc[:, ['up']].values.astype('float32'))\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "inputs, targets = df_to_tensor(df)\n",
    "print(inputs[0])\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a training and validation dataset\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch uses dataloaders to load data in batches\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, 1, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use gpu if avaliable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_NN(\n",
       "  (model): Sequential(\n",
       "    (0): LSTM(84, 256, num_layers=2, batch_first=True)\n",
       "    (1): extract_tensor()\n",
       "    (2): Linear(in_features=256, out_features=1028, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): Linear(in_features=1028, out_features=512, bias=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (9): GELU(approximate='none')\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class extract_tensor(nn.Module):\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        tensor, _ = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        return tensor#[:, -1, :]\n",
    "\n",
    "class LSTM_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.LSTM(input_size, 256, num_layers, batch_first=True),\n",
    "            extract_tensor(),\n",
    "            nn.Linear(256, 1028),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1028, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# input size is 84 because we have 84 columns in our dataframe\n",
    "# output size is 1 because we are predicting up=1 or down=0\n",
    "input_size = 84\n",
    "output_size = 1\n",
    "hidden_size = 84\n",
    "num_layers = 2\n",
    "model = LSTM_NN(input_size, hidden_size, num_layers, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 200\n",
    "# num_epochs = 300 # I originally ran this for 100 epochs but had to continue running to decrease the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg_loss: 0.364333311740256\n",
      "epoch: 10, avg_loss: 0.24216613526592887\n",
      "epoch: 20, avg_loss: 0.20421121251809088\n",
      "epoch: 30, avg_loss: 0.1914132426700321\n",
      "epoch: 40, avg_loss: 0.17902690302845425\n",
      "epoch: 50, avg_loss: 0.17075644896069975\n",
      "epoch: 60, avg_loss: 0.16400598021338902\n",
      "epoch: 70, avg_loss: 0.15400713194885526\n",
      "epoch: 80, avg_loss: 0.14483961907890736\n",
      "epoch: 90, avg_loss: 0.13923213807468732\n",
      "epoch: 100, avg_loss: 0.1310677132017522\n",
      "epoch: 110, avg_loss: 0.1230569335133261\n",
      "epoch: 120, avg_loss: 0.11616762951722642\n",
      "epoch: 130, avg_loss: 0.11471157199681088\n",
      "epoch: 140, avg_loss: 0.1086859085654478\n",
      "epoch: 150, avg_loss: 0.10643907343324327\n",
      "epoch: 160, avg_loss: 0.10065736953539871\n",
      "epoch: 170, avg_loss: 0.09677956880057027\n",
      "epoch: 180, avg_loss: 0.09352900500969864\n",
      "epoch: 190, avg_loss: 0.08617471416273388\n",
      "epoch: 200, avg_loss: 0.08089552406592392\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "training_losses = []\n",
    "sequence_length = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        # inputs = inputs.view(-1, sequence_length, input_size)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        # outputs = torch.sigmoid(outputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    #average the loss over all batches\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    training_losses.append(avg_loss)\n",
    "    if(epoch % 10 == 0 or epoch == 1):\n",
    "        print(f'epoch: {epoch}, avg_loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], device='cuda:0')\n",
      "tensor([[0.]], device='cuda:0')\n",
      "val_loss: 0.07145683332024942, val_acc: 0.7832083680459132\n"
     ]
    }
   ],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    flag = 0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "     \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if flag == 0:\n",
    "                print(targets)\n",
    "                print(torch.round(torch.sigmoid(outputs)))\n",
    "                flag = 1\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return val_loss, accuracy\n",
    "\n",
    "val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "print(f'val_loss: {val_loss}, val_acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8384, 0.6218, 0.0964, 0.1332, 0.7610, 0.1500, 0.8000, 0.8250, 0.3453,\n",
      "        0.9183, 0.9200, 0.9199, 0.7466, 0.7152, 0.5881, 0.9176, 0.1422, 0.9272,\n",
      "        0.1510, 0.6481, 0.4280, 0.4814, 0.4289, 0.0998, 0.9250, 1.0000, 0.9338,\n",
      "        0.7940, 0.7269, 0.6243, 0.7610, 0.7181, 0.5339, 0.9604, 0.9982, 0.7262,\n",
      "        0.9215, 0.9080, 0.3292, 0.2287, 0.7729, 0.0916, 0.9254, 0.4240, 0.3633,\n",
      "        0.5953, 0.5943, 0.5943, 0.5881, 0.8577, 0.9200, 0.5049, 0.5539, 0.7208,\n",
      "        0.5049, 0.8912, 0.3345, 0.9084, 0.9210, 0.1042, 0.9145, 0.6438, 0.7618,\n",
      "        0.9248, 0.9222, 0.7811, 0.7888, 0.7927, 0.7887, 0.9190, 0.0442, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "LSTM: Expected input to be 2-D or 3-D but received 1-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(inputs[\u001b[39mlen\u001b[39m(inputs)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model(inputs[\u001b[39mlen\u001b[39;49m(inputs)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mLSTM_NN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:773\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     batch_sizes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 773\u001b[0m     \u001b[39massert\u001b[39;00m (\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTM: Expected input to be 2-D or 3-D but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    775\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: LSTM: Expected input to be 2-D or 3-D but received 1-D tensor"
     ]
    }
   ],
   "source": [
    "#not yet working for LSTM model\n",
    "\n",
    "# how will visa do tomorrow? > 0.5 = up, < 0.5 = down\n",
    "inputs, targets = df_to_tensor(df)\n",
    "inputs = inputs.to(device)\n",
    "print(inputs[len(inputs)-1])\n",
    "model(inputs[len(inputs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(\"market_data/merged_data/\") if f.endswith('.csv')]\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f\"{filenames[i]}: {model(inputs[idx]).item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
