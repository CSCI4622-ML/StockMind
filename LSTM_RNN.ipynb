{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Normalizes all columns in a pandas DataFrame  using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The normalized DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = [col for col in df.columns]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>WMA</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. adjusted close</th>\n",
       "      <th>6. volume</th>\n",
       "      <th>7. dividend amount</th>\n",
       "      <th>8. split coefficient</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43059</th>\n",
       "      <td>0.925470</td>\n",
       "      <td>0.543459</td>\n",
       "      <td>0.271353</td>\n",
       "      <td>0.215465</td>\n",
       "      <td>0.558804</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.512792</td>\n",
       "      <td>0.961274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937124</td>\n",
       "      <td>0.717283</td>\n",
       "      <td>0.724516</td>\n",
       "      <td>0.725372</td>\n",
       "      <td>0.724509</td>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.085269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43060</th>\n",
       "      <td>0.924521</td>\n",
       "      <td>0.546146</td>\n",
       "      <td>0.280797</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>0.602606</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.498271</td>\n",
       "      <td>0.970419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945408</td>\n",
       "      <td>0.718257</td>\n",
       "      <td>0.725454</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.721923</td>\n",
       "      <td>0.981716</td>\n",
       "      <td>0.054010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43061</th>\n",
       "      <td>0.920910</td>\n",
       "      <td>0.543406</td>\n",
       "      <td>0.278851</td>\n",
       "      <td>0.211492</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.497862</td>\n",
       "      <td>0.976177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951449</td>\n",
       "      <td>0.715121</td>\n",
       "      <td>0.718033</td>\n",
       "      <td>0.710737</td>\n",
       "      <td>0.703099</td>\n",
       "      <td>0.961788</td>\n",
       "      <td>0.077614</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43062</th>\n",
       "      <td>0.924132</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.277001</td>\n",
       "      <td>0.212026</td>\n",
       "      <td>0.734318</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.490675</td>\n",
       "      <td>0.983225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958346</td>\n",
       "      <td>0.708679</td>\n",
       "      <td>0.712659</td>\n",
       "      <td>0.711424</td>\n",
       "      <td>0.712511</td>\n",
       "      <td>0.972543</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43063</th>\n",
       "      <td>0.919373</td>\n",
       "      <td>0.549282</td>\n",
       "      <td>0.282166</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>0.813328</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.486867</td>\n",
       "      <td>0.992715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966062</td>\n",
       "      <td>0.728429</td>\n",
       "      <td>0.728909</td>\n",
       "      <td>0.730895</td>\n",
       "      <td>0.721923</td>\n",
       "      <td>0.983297</td>\n",
       "      <td>0.134807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  \\\n",
       "43059     0.925470  0.543459  0.271353  0.215465  0.558804        0.65   \n",
       "43060     0.924521  0.546146  0.280797  0.216888  0.602606        0.60   \n",
       "43061     0.920910  0.543406  0.278851  0.211492  0.651815        0.55   \n",
       "43062     0.924132  0.552326  0.277001  0.212026  0.734318        0.50   \n",
       "43063     0.919373  0.549282  0.282166  0.216814  0.813328        0.45   \n",
       "\n",
       "       Aroon Up  AROONOSC       ATR  Real Upper Band  ...       WMA   1. open  \\\n",
       "43059      0.95      0.65  0.512792         0.961274  ...  0.937124  0.717283   \n",
       "43060      0.90      0.65  0.498271         0.970419  ...  0.945408  0.718257   \n",
       "43061      0.85      0.65  0.497862         0.976177  ...  0.951449  0.715121   \n",
       "43062      0.80      0.65  0.490675         0.983225  ...  0.958346  0.708679   \n",
       "43063      0.75      0.65  0.486867         0.992715  ...  0.966062  0.728429   \n",
       "\n",
       "        2. high    3. low  4. close  5. adjusted close  6. volume  \\\n",
       "43059  0.724516  0.725372  0.724509           0.984667   0.085269   \n",
       "43060  0.725454  0.730207  0.721923           0.981716   0.054010   \n",
       "43061  0.718033  0.710737  0.703099           0.961788   0.077614   \n",
       "43062  0.712659  0.711424  0.712511           0.972543   0.066541   \n",
       "43063  0.728909  0.730895  0.721923           0.983297   0.134807   \n",
       "\n",
       "       7. dividend amount  8. split coefficient  company  \n",
       "43059            0.000000                   0.0       10  \n",
       "43060            0.000000                   0.0       10  \n",
       "43061            0.666667                   0.0       10  \n",
       "43062            0.000000                   0.0       10  \n",
       "43063            0.000000                   0.0       10  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_csvs_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder into a single pandas DataFrame also normalizes before combining them.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the concatenated data from all CSV files in the input folder.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension to read all CSV files in the folder into a list of DataFrames.\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # Use a list comprehension to get the filenames of all CSV files in the folder.\n",
    "    filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    processed_dfs = []\n",
    "    i = 0\n",
    "    for df, filename in zip(dfs, filenames):\n",
    "        # Dont need the date column\n",
    "        df = df.drop(['date'], axis=1)\n",
    "        # normalize the dataframes before combining them\n",
    "        df = normalize_dataframe(df)\n",
    "        # for the neural network to understand the company name we need to convert it to a number\n",
    "        df['company'] = i\n",
    "        i += 1\n",
    "        processed_dfs.append(df)\n",
    "    combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "df = combine_csvs_from_folder('market_data/merged_data')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need this for later\n",
    "def find_indices_of_last_company_changes(df):\n",
    "    indices = []\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, 'company'] != df.loc[i - 1, 'company']:\n",
    "            indices.append(i-1)\n",
    "    return indices\n",
    "idxs = find_indices_of_last_company_changes(df)\n",
    "idxs.append(len(df) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should one hot encode the company column\n",
    "# first we need to change it to a string so we can one hot encode it\n",
    "df['company'] = df['company'].astype(str)\n",
    "df = pd.get_dummies(df, columns=['company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>company_10</th>\n",
       "      <th>company_2</th>\n",
       "      <th>company_3</th>\n",
       "      <th>company_4</th>\n",
       "      <th>company_5</th>\n",
       "      <th>company_6</th>\n",
       "      <th>company_7</th>\n",
       "      <th>company_8</th>\n",
       "      <th>company_9</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176768</td>\n",
       "      <td>0.545425</td>\n",
       "      <td>0.154470</td>\n",
       "      <td>0.101243</td>\n",
       "      <td>0.431009</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.061018</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.174206</td>\n",
       "      <td>0.532056</td>\n",
       "      <td>0.164471</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.427970</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.061018</td>\n",
       "      <td>0.017519</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.178504</td>\n",
       "      <td>0.555077</td>\n",
       "      <td>0.164130</td>\n",
       "      <td>0.114041</td>\n",
       "      <td>0.427821</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.063319</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.182646</td>\n",
       "      <td>0.590061</td>\n",
       "      <td>0.159311</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.426002</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.062719</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178573</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.149749</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.423586</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.063219</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  Aroon Up  \\\n",
       "0     0.176768  0.545425  0.154470  0.101243  0.431009        0.95      0.25   \n",
       "1     0.174206  0.532056  0.164471  0.106557  0.427970        0.90      0.20   \n",
       "2     0.178504  0.555077  0.164130  0.114041  0.427821        0.85      0.15   \n",
       "3     0.182646  0.590061  0.159311  0.123042  0.426002        0.80      0.10   \n",
       "4     0.178573  0.583195  0.149749  0.131813  0.423586        0.75      0.05   \n",
       "\n",
       "   AROONOSC       ATR  Real Upper Band  ...  company_10  company_2  company_3  \\\n",
       "0      0.15  0.061018         0.017387  ...           0          0          0   \n",
       "1      0.15  0.061018         0.017519  ...           0          0          0   \n",
       "2      0.15  0.063319         0.017517  ...           0          0          0   \n",
       "3      0.15  0.062719         0.017436  ...           0          0          0   \n",
       "4      0.15  0.063219         0.017113  ...           0          0          0   \n",
       "\n",
       "   company_4  company_5  company_6  company_7  company_8  company_9  up  \n",
       "0          0          0          0          0          0          0   0  \n",
       "1          0          0          0          0          0          0   0  \n",
       "2          0          0          0          0          0          0   1  \n",
       "3          0          0          0          0          0          0   1  \n",
       "4          0          0          0          0          0          0   0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_up_column(df):\n",
    "    # Create empty 'up' and 'down' columns\n",
    "    df['up'] = 0\n",
    "    \n",
    "    # Loop over the rows (skipping the first row)\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, '4. close'] > df.loc[i-1, '4. close']:\n",
    "            df.loc[i, 'up'] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "df = add_up_column(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1768, 0.5454, 0.1545, 0.1012, 0.4310, 0.9500, 0.2500, 0.1500, 0.0610,\n",
      "        0.0174, 0.0162, 0.0184, 0.6001, 0.3041, 0.3204, 0.0149, 0.3287, 0.0154,\n",
      "        0.0770, 0.1221, 0.3806, 0.3714, 0.4913, 0.8474, 0.0167, 1.0000, 0.0145,\n",
      "        0.4291, 0.4456, 0.4205, 0.4310, 0.4336, 0.3848, 0.0077, 0.0000, 0.4928,\n",
      "        0.0158, 0.0159, 0.4145, 0.0368, 0.4311, 0.3511, 0.2082, 0.1884, 0.0262,\n",
      "        0.7284, 0.5345, 0.5344, 0.3204, 0.0190, 0.0162, 0.1799, 0.2654, 0.3207,\n",
      "        0.1799, 0.2269, 0.0756, 0.0171, 0.0140, 0.0221, 0.0167, 0.7760, 0.3570,\n",
      "        0.1964, 0.0155, 0.1429, 0.1544, 0.1394, 0.1461, 0.0132, 0.0213, 0.0000,\n",
      "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000])\n",
      "torch.Size([43064, 84])\n",
      "torch.Size([43064, 1])\n"
     ]
    }
   ],
   "source": [
    "# neural networks require tensors, so we need to convert our dataframes to tensors\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    inputs_columns = df.columns[df.columns != 'up']\n",
    "    inputs = torch.from_numpy(df.loc[:, inputs_columns].values.astype('float32'))\n",
    "    targets = torch.from_numpy(df.loc[:, ['up']].values.astype('float32'))\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "inputs, targets = df_to_tensor(df)\n",
    "print(inputs[0])\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(inputs, targets, seq_length):\n",
    "    seq_inputs = []\n",
    "    seq_targets = []\n",
    "    for i in range(len(inputs) - seq_length):\n",
    "        seq_inputs.append(inputs[i:i + seq_length])\n",
    "        seq_targets.append(targets[i + seq_length])\n",
    "    return torch.stack(seq_inputs), torch.stack(seq_targets)\n",
    "\n",
    "sequence_length = 10\n",
    "seq_inputs, seq_targets  = create_sequences(inputs, targets, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a training and validation dataset\n",
    "\n",
    "dataset = TensorDataset(seq_inputs, seq_targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch uses dataloaders to load data in batches\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, 1, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use gpu if avaliable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_NN(\n",
       "  (lstm): LSTM(84, 84, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=84, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# input size is 84 because we have 84 columns in our dataframe\n",
    "# output size is 1 because we are predicting up=1 or down=0\n",
    "input_size = 84\n",
    "output_size = 1\n",
    "hidden_size = 84\n",
    "num_layers = 2\n",
    "model = LSTM_NN(input_size, hidden_size, num_layers, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg_loss: 0.6929363894039358\n",
      "epoch: 10, avg_loss: 0.6895449062776283\n",
      "epoch: 20, avg_loss: 0.6789701066073581\n",
      "epoch: 30, avg_loss: 0.6507334395273198\n",
      "epoch: 40, avg_loss: 0.5876412201209886\n",
      "epoch: 50, avg_loss: 0.4881404682376681\n",
      "epoch: 60, avg_loss: 0.38946381458163964\n",
      "epoch: 70, avg_loss: 0.30170433099805954\n",
      "epoch: 80, avg_loss: 0.23716625897489355\n",
      "epoch: 90, avg_loss: 0.19536092947926043\n",
      "epoch: 100, avg_loss: 0.1829669413834634\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "training_losses = []\n",
    "sequence_length = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.view(-1, sequence_length, input_size)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    #average the loss over all batches\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    training_losses.append(avg_loss)\n",
    "    if(epoch % 10 == 0 or epoch == 1):\n",
    "        print(f'epoch: {epoch}, avg_loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], device='cuda:0')\n",
      "tensor([[1.]], device='cuda:0')\n",
      "val_loss: 0.17529431852307104, val_acc: 0.5437231448147718\n"
     ]
    }
   ],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    flag = 0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.view(-1, sequence_length, input_size)\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # Move data to GPU if available\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, targets.float()) # BCE loss expects float inputs\n",
    "            val_loss += loss.item() * inputs.size(0) # Track total validation loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if flag == 0:\n",
    "                print(targets)\n",
    "                print(torch.round(torch.sigmoid(outputs)))\n",
    "                flag = 1\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return val_loss, accuracy\n",
    "\n",
    "val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "print(f'val_loss: {val_loss}, val_acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8384, 0.6218, 0.0964, 0.1332, 0.7610, 0.1500, 0.8000, 0.8250, 0.3453,\n",
      "        0.9183, 0.9200, 0.9199, 0.7466, 0.7152, 0.5881, 0.9176, 0.1422, 0.9272,\n",
      "        0.1510, 0.6481, 0.4280, 0.4814, 0.4289, 0.0998, 0.9250, 1.0000, 0.9338,\n",
      "        0.7940, 0.7269, 0.6243, 0.7610, 0.7181, 0.5339, 0.9604, 0.9982, 0.7262,\n",
      "        0.9215, 0.9080, 0.3292, 0.2287, 0.7729, 0.0916, 0.9254, 0.4240, 0.3633,\n",
      "        0.5953, 0.5943, 0.5943, 0.5881, 0.8577, 0.9200, 0.5049, 0.5539, 0.7208,\n",
      "        0.5049, 0.8912, 0.3345, 0.9084, 0.9210, 0.1042, 0.9145, 0.6438, 0.7618,\n",
      "        0.9248, 0.9222, 0.7811, 0.7888, 0.7927, 0.7887, 0.9190, 0.0442, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "LSTM: Expected input to be 2-D or 3-D but received 1-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(inputs[\u001b[39mlen\u001b[39m(inputs)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model(inputs[\u001b[39mlen\u001b[39;49m(inputs)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mLSTM_NN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     13\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:773\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     batch_sizes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 773\u001b[0m     \u001b[39massert\u001b[39;00m (\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTM: Expected input to be 2-D or 3-D but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    775\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: LSTM: Expected input to be 2-D or 3-D but received 1-D tensor"
     ]
    }
   ],
   "source": [
    "#not yet working for LSTM model\n",
    "\n",
    "# how will visa do tomorrow? > 0.5 = up, < 0.5 = down\n",
    "inputs, targets = df_to_tensor(df)\n",
    "inputs = inputs.to(device)\n",
    "print(inputs[len(inputs)-1])\n",
    "model(inputs[len(inputs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(\"market_data/merged_data/\") if f.endswith('.csv')]\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f\"{filenames[i]}: {model(inputs[idx]).item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
