{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Normalizes all columns in a pandas DataFrame  using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The normalized DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = [col for col in df.columns]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>WMA</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. adjusted close</th>\n",
       "      <th>6. volume</th>\n",
       "      <th>7. dividend amount</th>\n",
       "      <th>8. split coefficient</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54006</th>\n",
       "      <td>0.828590</td>\n",
       "      <td>0.611121</td>\n",
       "      <td>0.124368</td>\n",
       "      <td>0.161278</td>\n",
       "      <td>0.721455</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.371731</td>\n",
       "      <td>0.908468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912402</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.792715</td>\n",
       "      <td>0.795367</td>\n",
       "      <td>0.785348</td>\n",
       "      <td>0.915660</td>\n",
       "      <td>0.053093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54007</th>\n",
       "      <td>0.831386</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>0.120646</td>\n",
       "      <td>0.147684</td>\n",
       "      <td>0.739753</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.361116</td>\n",
       "      <td>0.912497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915420</td>\n",
       "      <td>0.780175</td>\n",
       "      <td>0.785976</td>\n",
       "      <td>0.794163</td>\n",
       "      <td>0.787510</td>\n",
       "      <td>0.917801</td>\n",
       "      <td>0.047816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54008</th>\n",
       "      <td>0.832392</td>\n",
       "      <td>0.613550</td>\n",
       "      <td>0.109766</td>\n",
       "      <td>0.139020</td>\n",
       "      <td>0.741281</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.358681</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917420</td>\n",
       "      <td>0.777547</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.786126</td>\n",
       "      <td>0.778268</td>\n",
       "      <td>0.908648</td>\n",
       "      <td>0.041491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54009</th>\n",
       "      <td>0.836382</td>\n",
       "      <td>0.618820</td>\n",
       "      <td>0.098034</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.750070</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.349887</td>\n",
       "      <td>0.916590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919478</td>\n",
       "      <td>0.775004</td>\n",
       "      <td>0.777489</td>\n",
       "      <td>0.784578</td>\n",
       "      <td>0.780133</td>\n",
       "      <td>0.910496</td>\n",
       "      <td>0.049474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54010</th>\n",
       "      <td>0.838405</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.096450</td>\n",
       "      <td>0.133156</td>\n",
       "      <td>0.760952</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.345329</td>\n",
       "      <td>0.918329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922155</td>\n",
       "      <td>0.781107</td>\n",
       "      <td>0.788834</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.788697</td>\n",
       "      <td>0.918977</td>\n",
       "      <td>0.044220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  \\\n",
       "54006     0.828590  0.611121  0.124368  0.161278  0.721455        0.35   \n",
       "54007     0.831386  0.613370  0.120646  0.147684  0.739753        0.30   \n",
       "54008     0.832392  0.613550  0.109766  0.139020  0.741281        0.25   \n",
       "54009     0.836382  0.618820  0.098034  0.135600  0.750070        0.20   \n",
       "54010     0.838405  0.621768  0.096450  0.133156  0.760952        0.15   \n",
       "\n",
       "       Aroon Up  AROONOSC       ATR  Real Upper Band  ...       WMA   1. open  \\\n",
       "54006      1.00     0.825  0.371731         0.908468  ...  0.912402  0.789583   \n",
       "54007      0.95     0.825  0.361116         0.912497  ...  0.915420  0.780175   \n",
       "54008      0.90     0.825  0.358681         0.914832  ...  0.917420  0.777547   \n",
       "54009      0.85     0.825  0.349887         0.916590  ...  0.919478  0.775004   \n",
       "54010      0.80     0.825  0.345329         0.918329  ...  0.922155  0.781107   \n",
       "\n",
       "        2. high    3. low  4. close  5. adjusted close  6. volume  \\\n",
       "54006  0.792715  0.795367  0.785348           0.915660   0.053093   \n",
       "54007  0.785976  0.794163  0.787510           0.917801   0.047816   \n",
       "54008  0.778555  0.786126  0.778268           0.908648   0.041491   \n",
       "54009  0.777489  0.784578  0.780133           0.910496   0.049474   \n",
       "54010  0.788834  0.792659  0.788697           0.918977   0.044220   \n",
       "\n",
       "       7. dividend amount  8. split coefficient  company  \n",
       "54006                 0.0                   0.0       10  \n",
       "54007                 0.0                   0.0       10  \n",
       "54008                 0.0                   0.0       10  \n",
       "54009                 0.0                   0.0       10  \n",
       "54010                 0.0                   0.0       10  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_csvs_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder into a single pandas DataFrame also normalizes before combining them.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the concatenated data from all CSV files in the input folder.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension to read all CSV files in the folder into a list of DataFrames.\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # Use a list comprehension to get the filenames of all CSV files in the folder.\n",
    "    filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    processed_dfs = []\n",
    "    i = 0\n",
    "    for df, filename in zip(dfs, filenames):\n",
    "        # Dont need the date column\n",
    "        df = df.drop(['date'], axis=1)\n",
    "        # normalize the dataframes before combining them\n",
    "        df = normalize_dataframe(df)\n",
    "        # for the neural network to understand the company name we need to convert it to a number\n",
    "        df['company'] = i\n",
    "        i += 1\n",
    "        processed_dfs.append(df)\n",
    "    combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "df = combine_csvs_from_folder('market_data/merged_data')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need this for later\n",
    "def find_indices_of_last_company_changes(df):\n",
    "    indices = []\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, 'company'] != df.loc[i - 1, 'company']:\n",
    "            indices.append(i-1)\n",
    "    return indices\n",
    "idxs = find_indices_of_last_company_changes(df)\n",
    "idxs.append(len(df) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should one hot encode the company column\n",
    "# first we need to change it to a string so we can one hot encode it\n",
    "df['company'] = df['company'].astype(str)\n",
    "df = pd.get_dummies(df, columns=['company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>company_10</th>\n",
       "      <th>company_2</th>\n",
       "      <th>company_3</th>\n",
       "      <th>company_4</th>\n",
       "      <th>company_5</th>\n",
       "      <th>company_6</th>\n",
       "      <th>company_7</th>\n",
       "      <th>company_8</th>\n",
       "      <th>company_9</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172135</td>\n",
       "      <td>0.545425</td>\n",
       "      <td>0.154470</td>\n",
       "      <td>0.101243</td>\n",
       "      <td>0.502764</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.169640</td>\n",
       "      <td>0.532056</td>\n",
       "      <td>0.164471</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.502062</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.173826</td>\n",
       "      <td>0.555077</td>\n",
       "      <td>0.164130</td>\n",
       "      <td>0.114041</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.011184</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.177859</td>\n",
       "      <td>0.590061</td>\n",
       "      <td>0.159311</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.501608</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.173893</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.149749</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.501044</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  Aroon Up  \\\n",
       "0     0.172135  0.545425  0.154470  0.101243  0.502764        0.95      0.25   \n",
       "1     0.169640  0.532056  0.164471  0.106557  0.502062        0.90      0.20   \n",
       "2     0.173826  0.555077  0.164130  0.114041  0.502022        0.85      0.15   \n",
       "3     0.177859  0.590061  0.159311  0.123042  0.501608        0.80      0.10   \n",
       "4     0.173893  0.583195  0.149749  0.131813  0.501044        0.75      0.05   \n",
       "\n",
       "   AROONOSC       ATR  Real Upper Band  ...  company_10  company_2  company_3  \\\n",
       "0      0.15  0.010778         0.004883  ...           0          0          0   \n",
       "1      0.15  0.010778         0.004920  ...           0          0          0   \n",
       "2      0.15  0.011184         0.004920  ...           0          0          0   \n",
       "3      0.15  0.011078         0.004897  ...           0          0          0   \n",
       "4      0.15  0.011167         0.004806  ...           0          0          0   \n",
       "\n",
       "   company_4  company_5  company_6  company_7  company_8  company_9  up  \n",
       "0          0          0          0          0          0          0   0  \n",
       "1          0          0          0          0          0          0   0  \n",
       "2          0          0          0          0          0          0   1  \n",
       "3          0          0          0          0          0          0   1  \n",
       "4          0          0          0          0          0          0   0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_up_column(df):\n",
    "    # Create empty 'up' and 'down' columns\n",
    "    df['up'] = 0\n",
    "    \n",
    "    # Loop over the rows (skipping the first row)\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, '4. close'] > df.loc[i-1, '4. close']:\n",
    "            df.loc[i, 'up'] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "df = add_up_column(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1721, 0.5454, 0.1545, 0.1012, 0.5028, 0.9500, 0.2500, 0.1500, 0.0108,\n",
      "        0.0049, 0.0044, 0.0048, 0.6001, 0.3041, 0.3204, 0.0041, 0.3287, 0.0043,\n",
      "        0.0770, 0.1221, 0.4935, 0.5869, 0.4913, 0.8474, 0.0045, 1.0000, 0.0041,\n",
      "        0.4687, 0.4572, 0.5075, 0.5028, 0.4830, 0.5257, 0.0021, 0.0000, 0.4928,\n",
      "        0.0043, 0.0044, 0.4145, 0.0082, 0.4762, 0.3511, 0.2082, 0.1884, 0.0058,\n",
      "        0.7284, 0.5345, 0.5344, 0.3204, 0.0051, 0.0044, 0.1780, 0.2642, 0.3207,\n",
      "        0.1780, 0.2269, 0.0756, 0.0045, 0.0039, 0.0067, 0.0046, 0.7760, 0.3548,\n",
      "        0.1964, 0.0043, 0.1429, 0.1544, 0.1394, 0.1461, 0.0037, 0.0121, 0.0000,\n",
      "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000])\n",
      "torch.Size([54011, 84])\n",
      "torch.Size([54011, 1])\n"
     ]
    }
   ],
   "source": [
    "# neural networks require tensors, so we need to convert our dataframes to tensors\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    inputs_columns = df.columns[df.columns != 'up']\n",
    "    inputs = torch.from_numpy(df.loc[:, inputs_columns].values.astype('float32'))\n",
    "    targets = torch.from_numpy(df.loc[:, ['up']].values.astype('float32'))\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "inputs, targets = df_to_tensor(df)\n",
    "print(inputs[0])\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(inputs, targets, seq_length):\n",
    "    seq_inputs = []\n",
    "    seq_targets = []\n",
    "    for i in range(len(inputs) - seq_length):\n",
    "        seq_inputs.append(inputs[i:i + seq_length])\n",
    "        seq_targets.append(targets[i + seq_length])\n",
    "    return torch.stack(seq_inputs), torch.stack(seq_targets)\n",
    "\n",
    "sequence_length = 10\n",
    "seq_inputs, seq_targets  = create_sequences(inputs, targets, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a training and validation dataset\n",
    "\n",
    "dataset = TensorDataset(seq_inputs, seq_targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch uses dataloaders to load data in batches\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, 1, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use gpu if avaliable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_NN(\n",
       "  (lstm): LSTM(84, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# input size is 84 because we have 84 columns in our dataframe\n",
    "# output size is 1 because we are predicting up=1 or down=0\n",
    "input_size = 84\n",
    "output_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "model = LSTM_NN(input_size, hidden_size, num_layers, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg_loss: 0.6928524434284011\n",
      "epoch: 10, avg_loss: 0.6889105385513667\n",
      "epoch: 20, avg_loss: 0.6738405979075138\n",
      "epoch: 30, avg_loss: 0.5937126016164843\n",
      "epoch: 40, avg_loss: 0.3875538220902755\n",
      "epoch: 50, avg_loss: 0.19136141339467036\n",
      "epoch: 60, avg_loss: 0.07966352238262434\n",
      "epoch: 70, avg_loss: 0.04432207772340537\n",
      "epoch: 80, avg_loss: 0.03536153976227294\n",
      "epoch: 90, avg_loss: 0.02941232717317922\n",
      "epoch: 100, avg_loss: 0.033806618245680466\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "training_losses = []\n",
    "sequence_length = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.view(-1, sequence_length, input_size)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    #average the loss over all batches\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    training_losses.append(avg_loss)\n",
    "    if(epoch % 10 == 0 or epoch == 1):\n",
    "        print(f'epoch: {epoch}, avg_loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], device='cuda:0')\n",
      "tensor([[1.]], device='cuda:0')\n",
      "val_loss: 0.039295825328585554, val_acc: 0.5938339042681233\n"
     ]
    }
   ],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    flag = 0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.view(-1, sequence_length, input_size)\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # Move data to GPU if available\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, targets.float()) # BCE loss expects float inputs\n",
    "            val_loss += loss.item() * inputs.size(0) # Track total validation loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if flag == 0:\n",
    "                print(targets)\n",
    "                print(torch.round(torch.sigmoid(outputs)))\n",
    "                flag = 1\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return val_loss, accuracy\n",
    "\n",
    "val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "print(f'val_loss: {val_loss}, val_acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8384, 0.6218, 0.0964, 0.1332, 0.7610, 0.1500, 0.8000, 0.8250, 0.3453,\n",
      "        0.9183, 0.9200, 0.9199, 0.7466, 0.7152, 0.5881, 0.9176, 0.1422, 0.9272,\n",
      "        0.1510, 0.6481, 0.4280, 0.4814, 0.4289, 0.0998, 0.9250, 1.0000, 0.9338,\n",
      "        0.7940, 0.7269, 0.6243, 0.7610, 0.7181, 0.5339, 0.9604, 0.9982, 0.7262,\n",
      "        0.9215, 0.9080, 0.3292, 0.2287, 0.7729, 0.0916, 0.9254, 0.4240, 0.3633,\n",
      "        0.5953, 0.5943, 0.5943, 0.5881, 0.8577, 0.9200, 0.5049, 0.5539, 0.7208,\n",
      "        0.5049, 0.8912, 0.3345, 0.9084, 0.9210, 0.1042, 0.9145, 0.6438, 0.7618,\n",
      "        0.9248, 0.9222, 0.7811, 0.7888, 0.7927, 0.7887, 0.9190, 0.0442, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "LSTM: Expected input to be 2-D or 3-D but received 1-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(inputs[\u001b[39mlen\u001b[39m(inputs)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model(inputs[\u001b[39mlen\u001b[39;49m(inputs)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mLSTM_NN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     13\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:773\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     batch_sizes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 773\u001b[0m     \u001b[39massert\u001b[39;00m (\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTM: Expected input to be 2-D or 3-D but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    775\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: LSTM: Expected input to be 2-D or 3-D but received 1-D tensor"
     ]
    }
   ],
   "source": [
    "#not yet working for LSTM model\n",
    "\n",
    "# how will visa do tomorrow? > 0.5 = up, < 0.5 = down\n",
    "inputs, targets = df_to_tensor(df)\n",
    "inputs = inputs.to(device)\n",
    "print(inputs[len(inputs)-1])\n",
    "model(inputs[len(inputs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(\"market_data/merged_data/\") if f.endswith('.csv')]\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f\"{filenames[i]}: {model(inputs[idx]).item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
