{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Normalizes all columns in a pandas DataFrame  using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The normalized DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = [col for col in df.columns]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>WMA</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. adjusted close</th>\n",
       "      <th>6. volume</th>\n",
       "      <th>7. dividend amount</th>\n",
       "      <th>8. split coefficient</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54006</th>\n",
       "      <td>7.152134e+08</td>\n",
       "      <td>5.281559e+06</td>\n",
       "      <td>11.5380</td>\n",
       "      <td>12.9553</td>\n",
       "      <td>1.7529</td>\n",
       "      <td>35.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0997</td>\n",
       "      <td>228.1603</td>\n",
       "      <td>...</td>\n",
       "      <td>222.3566</td>\n",
       "      <td>229.00</td>\n",
       "      <td>230.05</td>\n",
       "      <td>226.83</td>\n",
       "      <td>227.66</td>\n",
       "      <td>227.66</td>\n",
       "      <td>5254725.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54007</th>\n",
       "      <td>7.185228e+08</td>\n",
       "      <td>5.706147e+06</td>\n",
       "      <td>11.3947</td>\n",
       "      <td>12.5320</td>\n",
       "      <td>2.2713</td>\n",
       "      <td>30.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.9907</td>\n",
       "      <td>229.1161</td>\n",
       "      <td>...</td>\n",
       "      <td>223.0567</td>\n",
       "      <td>226.78</td>\n",
       "      <td>228.47</td>\n",
       "      <td>226.55</td>\n",
       "      <td>228.17</td>\n",
       "      <td>228.17</td>\n",
       "      <td>4813750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54008</th>\n",
       "      <td>7.197143e+08</td>\n",
       "      <td>5.740214e+06</td>\n",
       "      <td>10.9759</td>\n",
       "      <td>12.2622</td>\n",
       "      <td>2.3146</td>\n",
       "      <td>25.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.9657</td>\n",
       "      <td>229.6702</td>\n",
       "      <td>...</td>\n",
       "      <td>223.5208</td>\n",
       "      <td>226.16</td>\n",
       "      <td>226.73</td>\n",
       "      <td>224.68</td>\n",
       "      <td>225.99</td>\n",
       "      <td>225.99</td>\n",
       "      <td>4285206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54009</th>\n",
       "      <td>7.244374e+08</td>\n",
       "      <td>6.735109e+06</td>\n",
       "      <td>10.5243</td>\n",
       "      <td>12.1557</td>\n",
       "      <td>2.5636</td>\n",
       "      <td>20.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.8754</td>\n",
       "      <td>230.0872</td>\n",
       "      <td>...</td>\n",
       "      <td>223.9982</td>\n",
       "      <td>225.56</td>\n",
       "      <td>226.48</td>\n",
       "      <td>224.32</td>\n",
       "      <td>226.43</td>\n",
       "      <td>226.43</td>\n",
       "      <td>4952351.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54010</th>\n",
       "      <td>7.268322e+08</td>\n",
       "      <td>7.291812e+06</td>\n",
       "      <td>10.4633</td>\n",
       "      <td>12.0796</td>\n",
       "      <td>2.8719</td>\n",
       "      <td>15.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.8286</td>\n",
       "      <td>230.4999</td>\n",
       "      <td>...</td>\n",
       "      <td>224.6191</td>\n",
       "      <td>227.00</td>\n",
       "      <td>229.14</td>\n",
       "      <td>226.20</td>\n",
       "      <td>228.45</td>\n",
       "      <td>228.45</td>\n",
       "      <td>4513284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Chaikin A/D         ADOSC      ADX     ADXR     APO  Aroon Down  \\\n",
       "54006  7.152134e+08  5.281559e+06  11.5380  12.9553  1.7529        35.0   \n",
       "54007  7.185228e+08  5.706147e+06  11.3947  12.5320  2.2713        30.0   \n",
       "54008  7.197143e+08  5.740214e+06  10.9759  12.2622  2.3146        25.0   \n",
       "54009  7.244374e+08  6.735109e+06  10.5243  12.1557  2.5636        20.0   \n",
       "54010  7.268322e+08  7.291812e+06  10.4633  12.0796  2.8719        15.0   \n",
       "\n",
       "       Aroon Up  AROONOSC     ATR  Real Upper Band  ...       WMA  1. open  \\\n",
       "54006     100.0      65.0  4.0997         228.1603  ...  222.3566   229.00   \n",
       "54007      95.0      65.0  3.9907         229.1161  ...  223.0567   226.78   \n",
       "54008      90.0      65.0  3.9657         229.6702  ...  223.5208   226.16   \n",
       "54009      85.0      65.0  3.8754         230.0872  ...  223.9982   225.56   \n",
       "54010      80.0      65.0  3.8286         230.4999  ...  224.6191   227.00   \n",
       "\n",
       "       2. high  3. low  4. close  5. adjusted close  6. volume  \\\n",
       "54006   230.05  226.83    227.66             227.66  5254725.0   \n",
       "54007   228.47  226.55    228.17             228.17  4813750.0   \n",
       "54008   226.73  224.68    225.99             225.99  4285206.0   \n",
       "54009   226.48  224.32    226.43             226.43  4952351.0   \n",
       "54010   229.14  226.20    228.45             228.45  4513284.0   \n",
       "\n",
       "       7. dividend amount  8. split coefficient  company  \n",
       "54006                 0.0                   1.0       10  \n",
       "54007                 0.0                   1.0       10  \n",
       "54008                 0.0                   1.0       10  \n",
       "54009                 0.0                   1.0       10  \n",
       "54010                 0.0                   1.0       10  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_csvs_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder into a single pandas DataFrame also normalizes before combining them.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the concatenated data from all CSV files in the input folder.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension to read all CSV files in the folder into a list of DataFrames.\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # Use a list comprehension to get the filenames of all CSV files in the folder.\n",
    "    filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    processed_dfs = []\n",
    "    i = 0\n",
    "    for df, filename in zip(dfs, filenames):\n",
    "        # Dont need the date column\n",
    "        df = df.drop(['date'], axis=1)\n",
    "        # normalize the dataframes before combining them\n",
    "        #df = normalize_dataframe(df)\n",
    "        # for the neural network to understand the company name we need to convert it to a number\n",
    "        df['company'] = i\n",
    "        i += 1\n",
    "        processed_dfs.append(df)\n",
    "    combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "df = combine_csvs_from_folder('market_data/merged_data')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need this for later\n",
    "def find_indices_of_last_company_changes(df):\n",
    "    indices = []\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, 'company'] != df.loc[i - 1, 'company']:\n",
    "            indices.append(i-1)\n",
    "    return indices\n",
    "idxs = find_indices_of_last_company_changes(df)\n",
    "idxs.append(len(df) - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should one hot encode the company column\n",
    "# first we need to change it to a string so we can one hot encode it\n",
    "df['company'] = df['company'].astype(str)\n",
    "df = pd.get_dummies(df, columns=['company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>company_10</th>\n",
       "      <th>company_2</th>\n",
       "      <th>company_3</th>\n",
       "      <th>company_4</th>\n",
       "      <th>company_5</th>\n",
       "      <th>company_6</th>\n",
       "      <th>company_7</th>\n",
       "      <th>company_8</th>\n",
       "      <th>company_9</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.307458e+08</td>\n",
       "      <td>-2.790035e+08</td>\n",
       "      <td>15.0266</td>\n",
       "      <td>13.1844</td>\n",
       "      <td>-0.0123</td>\n",
       "      <td>95.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>1.1249</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.725731e+08</td>\n",
       "      <td>-3.547090e+08</td>\n",
       "      <td>15.5542</td>\n",
       "      <td>13.4513</td>\n",
       "      <td>-0.0245</td>\n",
       "      <td>90.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>1.1317</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.668645e+08</td>\n",
       "      <td>-2.243441e+08</td>\n",
       "      <td>15.5362</td>\n",
       "      <td>13.8272</td>\n",
       "      <td>-0.0252</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>1.1316</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.240607e+08</td>\n",
       "      <td>-2.623278e+07</td>\n",
       "      <td>15.2820</td>\n",
       "      <td>14.2793</td>\n",
       "      <td>-0.0324</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>1.1274</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.603556e+08</td>\n",
       "      <td>-6.511672e+07</td>\n",
       "      <td>14.7776</td>\n",
       "      <td>14.7198</td>\n",
       "      <td>-0.0422</td>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>0.0695</td>\n",
       "      <td>1.1107</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Chaikin A/D         ADOSC      ADX     ADXR     APO  Aroon Down  Aroon Up  \\\n",
       "0 -4.307458e+08 -2.790035e+08  15.0266  13.1844 -0.0123        95.0      25.0   \n",
       "1 -6.725731e+08 -3.547090e+08  15.5542  13.4513 -0.0245        90.0      20.0   \n",
       "2 -2.668645e+08 -2.243441e+08  15.5362  13.8272 -0.0252        85.0      15.0   \n",
       "3  1.240607e+08 -2.623278e+07  15.2820  14.2793 -0.0324        80.0      10.0   \n",
       "4 -2.603556e+08 -6.511672e+07  14.7776  14.7198 -0.0422        75.0       5.0   \n",
       "\n",
       "   AROONOSC     ATR  Real Upper Band  ...  company_10  company_2  company_3  \\\n",
       "0     -70.0  0.0673           1.1249  ...           0          0          0   \n",
       "1     -70.0  0.0673           1.1317  ...           0          0          0   \n",
       "2     -70.0  0.0696           1.1316  ...           0          0          0   \n",
       "3     -70.0  0.0690           1.1274  ...           0          0          0   \n",
       "4     -70.0  0.0695           1.1107  ...           0          0          0   \n",
       "\n",
       "   company_4  company_5  company_6  company_7  company_8  company_9  up  \n",
       "0          0          0          0          0          0          0   0  \n",
       "1          0          0          0          0          0          0   0  \n",
       "2          0          0          0          0          0          0   1  \n",
       "3          0          0          0          0          0          0   1  \n",
       "4          0          0          0          0          0          0   0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_up_column(df):\n",
    "    # Create empty 'up' and 'down' columns\n",
    "    df['up'] = 0\n",
    "    \n",
    "    # Loop over the rows (skipping the first row)\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, '4. close'] > df.loc[i-1, '4. close']:\n",
    "            df.loc[i, 'up'] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "df = add_up_column(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.3075e+08, -2.7900e+08,  1.5027e+01,  1.3184e+01, -1.2300e-02,\n",
      "         9.5000e+01,  2.5000e+01, -7.0000e+01,  6.7300e-02,  1.1249e+00,\n",
      "         9.8730e-01,  8.4980e-01,  2.0090e-01, -1.5646e+02, -1.0042e+01,\n",
      "         9.4590e-01,  2.5578e+01,  9.5160e-01,  1.5876e+01, -9.9100e-01,\n",
      "        -2.7000e-02, -8.9000e-03, -1.7300e-02,  6.9480e-01,  9.9630e-01,\n",
      "         1.0000e+00,  9.2300e-01, -1.1000e-02,  1.2200e-02, -2.3200e-02,\n",
      "        -1.2300e-02,  3.0400e-02, -4.2700e-02,  5.6220e-01,  1.3320e-01,\n",
      "         5.4206e+01,  9.6290e-01,  9.6970e-01,  2.5903e+01,  3.4830e-01,\n",
      "        -5.8900e-02,  7.7768e+00,  7.8613e+09,  1.5351e+01,  2.0640e-01,\n",
      "        -1.2678e+00, -6.3755e+00,  9.3620e-01,  4.4979e+01,  1.1211e+00,\n",
      "         9.8730e-01,  1.8414e+01,  2.7301e+01,  3.2066e+01,  1.8414e+01,\n",
      "         2.2685e+01,  7.5618e+00,  1.0122e+00,  9.0810e-01,  8.7400e-02,\n",
      "         1.0052e+00,  3.6670e-01,  4.1021e+01, -8.0356e+01,  9.6420e-01,\n",
      "         1.1150e+02,  1.2000e+02,  1.0850e+02,  1.1381e+02,  8.6477e-01,\n",
      "         4.7306e+06,  0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])\n",
      "torch.Size([54011, 84])\n",
      "torch.Size([54011, 1])\n"
     ]
    }
   ],
   "source": [
    "# neural networks require tensors, so we need to convert our dataframes to tensors\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    inputs_columns = df.columns[df.columns != 'up']\n",
    "    inputs = torch.from_numpy(df.loc[:, inputs_columns].values.astype('float32'))\n",
    "    targets = torch.from_numpy(df.loc[:, ['up']].values.astype('float32'))\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "inputs, targets = df_to_tensor(df)\n",
    "print(inputs[0])\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(inputs, targets, seq_length):\n",
    "    seq_inputs = []\n",
    "    seq_targets = []\n",
    "    for i in range(len(inputs) - seq_length):\n",
    "        seq_inputs.append(inputs[i:i + seq_length])\n",
    "        seq_targets.append(targets[i + seq_length])\n",
    "    return torch.stack(seq_inputs), torch.stack(seq_targets)\n",
    "\n",
    "sequence_length = 10\n",
    "seq_inputs, seq_targets  = create_sequences(inputs, targets, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a training and validation dataset\n",
    "\n",
    "dataset = TensorDataset(seq_inputs, seq_targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch uses dataloaders to load data in batches\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, 12, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use gpu if avaliable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_NN(\n",
       "  (lstm): LSTM(84, 256, num_layers=5, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# input size is 84 because we have 84 columns in our dataframe\n",
    "# output size is 1 because we are predicting up=1 or down=0\n",
    "input_size = 84\n",
    "output_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 5\n",
    "model = LSTM_NN(input_size, hidden_size, num_layers, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg_loss: 0.6928209913850396\n",
      "epoch: 10, avg_loss: 0.6905814561798674\n",
      "epoch: 20, avg_loss: 0.6826905924562029\n",
      "epoch: 30, avg_loss: 0.6451379679390604\n",
      "epoch: 40, avg_loss: 0.5289174226505496\n",
      "epoch: 50, avg_loss: 0.36504797079551843\n",
      "epoch: 60, avg_loss: 0.22879056159353933\n",
      "epoch: 70, avg_loss: 0.13362696117134457\n",
      "epoch: 80, avg_loss: 0.06961475897103689\n",
      "epoch: 90, avg_loss: 0.05431623856638563\n",
      "epoch: 100, avg_loss: 0.03821887758282406\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "training_losses = []\n",
    "sequence_length = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.view(-1, sequence_length, input_size)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    #average the loss over all batches\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    training_losses.append(avg_loss)\n",
    "    if(epoch % 10 == 0 or epoch == 1):\n",
    "        print(f'epoch: {epoch}, avg_loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')\n",
      "tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.], device='cuda:0')\n",
      "val_loss: 0.031178818218975595, val_acc: 0.9900009258401999\n"
     ]
    }
   ],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    flag = 0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.view(-1, sequence_length, input_size)\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # Move data to GPU if available\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.float()) # BCE loss expects float inputs\n",
    "            val_loss += loss.item() * inputs.size(0) # Track total validation loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if flag == 0:\n",
    "                print(targets.view(-1))\n",
    "                print(torch.round(outputs.view(-1)))\n",
    "                flag = 1\n",
    "            predicted = torch.round(outputs)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return val_loss, accuracy\n",
    "\n",
    "val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "print(f'val_loss: {val_loss}, val_acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m inputs, targets \u001b[39m=\u001b[39m df_to_tensor(df)\n\u001b[0;32m      5\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model(inputs[\u001b[39mlen\u001b[39;49m(inputs)\u001b[39m-\u001b[39;49m\u001b[39m10\u001b[39;49m:\u001b[39mlen\u001b[39;49m(inputs)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m, in \u001b[0;36mLSTM_NN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 13\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     14\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m     15\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(out)\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\peter\\anaconda3\\envs\\alpaca\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:803\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    801\u001b[0m             msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malso be 2-D but got (\u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D, \u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D) tensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 803\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    804\u001b[0m         hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[0;32m    806\u001b[0m \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "#not yet working for LSTM model\n",
    "\n",
    "# how will visa do tomorrow? > 0.5 = up, < 0.5 = down\n",
    "inputs, targets = df_to_tensor(df)\n",
    "inputs = inputs.to(device)\n",
    "model(inputs[len(inputs)-10:len(inputs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0454e+08,  3.4130e+06,  1.3240e+01,  1.4752e+01, -1.3363e+00,\n",
       "          6.5000e+01,  2.5000e+01, -4.0000e+01,  4.2177e+00,  2.2580e+02,\n",
       "          2.1997e+02,  2.1413e+02, -2.9000e-01,  6.4738e+01,  3.2373e+00,\n",
       "          2.1975e+02,  1.9591e+00,  2.2054e+02,  1.8489e+01,  6.4948e+01,\n",
       "          1.2391e+00,  2.7758e+00,  9.0590e-01,  9.4000e-01,  2.1992e+02,\n",
       "          1.0000e+00,  2.2164e+02, -4.2300e-01, -9.1090e-01,  4.8790e-01,\n",
       "         -1.3363e+00, -2.0512e+00,  7.1490e-01,  2.1227e+02,  2.0731e+02,\n",
       "          6.0415e+01,  2.2061e+02,  2.1809e+02,  2.3910e+01,  2.0169e+01,\n",
       "          1.4600e+00,  1.9015e+00,  3.1344e+09,  2.2991e+01,  1.9394e+01,\n",
       "         -6.0690e-01,  6.6260e-01,  1.0066e+00,  5.1619e+01,  2.0957e+02,\n",
       "          2.1997e+02,  5.8390e+01,  6.7733e+01,  5.0857e+01,  5.8390e+01,\n",
       "          6.6700e+01,  7.7107e+01,  2.2131e+02,  2.1959e+02,  3.0400e+00,\n",
       "          2.1921e+02, -2.9600e-02,  4.8769e+01, -3.0046e+01,  2.1984e+02,\n",
       "          2.2259e+02,  2.2408e+02,  2.2139e+02,  2.2181e+02,  2.2181e+02,\n",
       "          7.2110e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.0273e+08,  1.6777e+06,  1.3009e+01,  1.4501e+01, -8.6290e-01,\n",
       "          6.0000e+01,  2.0000e+01, -4.0000e+01,  4.2408e+00,  2.2582e+02,\n",
       "          2.1999e+02,  2.1415e+02, -5.3210e-01,  2.2007e+01, -8.5760e-01,\n",
       "          2.1979e+02,  8.6209e+00,  2.2052e+02,  1.7788e+01,  8.7452e+01,\n",
       "          3.3313e+00,  2.6549e+00,  9.9900e-01,  7.3780e-01,  2.2002e+02,\n",
       "          0.0000e+00,  2.2163e+02, -3.9120e-01, -8.0690e-01,  4.1570e-01,\n",
       "         -8.6290e-01, -1.8193e+00,  9.5640e-01,  2.1235e+02,  2.0734e+02,\n",
       "          5.6339e+01,  2.2061e+02,  2.1809e+02,  2.5821e+01,  2.1900e+01,\n",
       "          3.9000e-01,  1.9247e+00,  3.1280e+09,  2.1722e+01,  1.8424e+01,\n",
       "         -3.9210e-01,  1.7730e-01,  1.0018e+00,  4.9571e+01,  2.0988e+02,\n",
       "          2.1999e+02,  4.3504e+01,  5.5543e+01,  2.9714e+01,  4.3504e+01,\n",
       "          4.2869e+00,  4.5202e+01,  2.2100e+02,  2.1969e+02,  4.6800e+00,\n",
       "          2.1915e+02, -3.0100e-02,  4.5909e+01, -3.7979e+01,  2.1987e+02,\n",
       "          2.2282e+02,  2.2333e+02,  2.1865e+02,  2.2033e+02,  2.2033e+02,\n",
       "          6.4186e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.0636e+08,  1.9705e+06,  1.2758e+01,  1.4247e+01, -2.3010e-01,\n",
       "          5.5000e+01,  1.5000e+01, -4.0000e+01,  4.1897e+00,  2.2619e+02,\n",
       "          2.2023e+02,  2.1427e+02,  3.8400e-01,  6.2295e+01,  6.9397e+00,\n",
       "          2.2036e+02,  8.0007e+00,  2.2079e+02,  1.7192e+01,  1.2130e+02,\n",
       "          3.3945e+00, -7.8820e-01,  8.5450e-01,  2.3680e-01,  2.1992e+02,\n",
       "          0.0000e+00,  2.2166e+02, -1.2490e-01, -6.7050e-01,  5.4560e-01,\n",
       "         -2.3010e-01, -1.5513e+00,  1.3212e+00,  2.1246e+02,  2.0736e+02,\n",
       "          5.9496e+01,  2.2061e+02,  2.1809e+02,  2.4829e+01,  2.0805e+01,\n",
       "          4.9400e+00,  1.8763e+00,  3.1325e+09,  2.1150e+01,  1.7723e+01,\n",
       "         -1.0450e-01,  2.2623e+00,  1.0226e+00,  5.3470e+01,  2.1019e+02,\n",
       "          2.2023e+02,  5.0905e+01,  5.0933e+01,  7.2143e+01,  5.0905e+01,\n",
       "          1.0000e+02,  5.6995e+01,  2.2075e+02,  2.2056e+02,  3.2200e+00,\n",
       "          2.1916e+02, -2.9200e-02,  5.1293e+01, -2.2058e+01,  2.2019e+02,\n",
       "          2.2229e+02,  2.2355e+02,  2.2092e+02,  2.2330e+02,  2.2330e+02,\n",
       "          4.4830e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.0507e+08,  1.5006e+06,  1.2421e+01,  1.3783e+01, -1.1800e-02,\n",
       "          5.0000e+01,  1.0000e+01, -4.0000e+01,  4.1315e+00,  2.2640e+02,\n",
       "          2.2040e+02,  2.1440e+02, -5.1570e-01,  5.7574e+01,  4.2545e+00,\n",
       "          2.2069e+02,  6.0166e+00,  2.2094e+02,  1.6713e+01,  1.4526e+02,\n",
       "          2.6671e+00, -1.4908e+00,  5.6980e-01, -1.7820e-01,  2.1981e+02,\n",
       "          0.0000e+00,  2.2167e+02,  1.0100e-02, -5.3440e-01,  5.4450e-01,\n",
       "         -1.1800e-02, -1.3029e+00,  1.2911e+00,  2.1256e+02,  2.0739e+02,\n",
       "          5.9480e+01,  2.2061e+02,  2.1809e+02,  2.3920e+01,  1.9765e+01,\n",
       "          3.3000e+00,  1.8580e+00,  3.1271e+09,  2.1205e+01,  1.7522e+01,\n",
       "         -5.4000e-03,  1.5064e+00,  1.0151e+00,  5.2127e+01,  2.1049e+02,\n",
       "          2.2040e+02,  5.6843e+01,  5.0417e+01,  6.8672e+01,  5.6843e+01,\n",
       "          6.5562e+01,  5.6616e+01,  2.2054e+02,  2.2102e+02,  3.0250e+00,\n",
       "          2.1924e+02, -2.7500e-02,  4.7858e+01, -2.7097e+01,  2.2039e+02,\n",
       "          2.2392e+02,  2.2424e+02,  2.2121e+02,  2.2236e+02,  2.2236e+02,\n",
       "          5.3839e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.1175e+08,  3.2958e+06,  1.1871e+01,  1.3462e+01,  5.7920e-01,\n",
       "          4.5000e+01,  5.0000e+00, -4.0000e+01,  4.0989e+00,  2.2671e+02,\n",
       "          2.2048e+02,  2.1425e+02,  7.2940e-01,  1.0780e+02,  1.1931e+01,\n",
       "          2.2153e+02,  1.4212e+00,  2.2137e+02,  1.6326e+01,  1.8206e+02,\n",
       "          2.0955e+00, -2.3614e+00, -3.6000e-02, -7.3210e-01,  2.1979e+02,\n",
       "          1.0000e+00,  2.2170e+02,  3.6310e-01, -3.5490e-01,  7.1800e-01,\n",
       "          5.7920e-01, -1.0283e+00,  1.6074e+00,  2.1269e+02,  2.0741e+02,\n",
       "          6.0443e+01,  2.2061e+02,  2.1809e+02,  2.2905e+01,  1.8777e+01,\n",
       "          1.6900e+00,  1.8180e+00,  3.1366e+09,  2.2263e+01,  1.8251e+01,\n",
       "          2.6280e-01,  7.5520e-01,  1.0076e+00,  5.5965e+01,  2.1079e+02,\n",
       "          2.2048e+02,  7.8510e+01,  6.2086e+01,  9.4715e+01,  7.8510e+01,\n",
       "          1.0000e+02,  8.8521e+01,  2.2040e+02,  2.2221e+02,  3.4800e+00,\n",
       "          2.1940e+02, -2.4200e-02,  5.6825e+01, -1.0480e+01,  2.2087e+02,\n",
       "          2.2360e+02,  2.2584e+02,  2.2329e+02,  2.2546e+02,  2.2546e+02,\n",
       "          9.5090e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.1776e+08,  5.6440e+06,  1.1644e+01,  1.3254e+01,  1.1782e+00,\n",
       "          4.0000e+01,  1.0000e+02,  6.0000e+01,  4.1460e+00,  2.2734e+02,\n",
       "          2.2059e+02,  2.1385e+02,  7.4800e-01,  1.6476e+02,  1.9673e+01,\n",
       "          2.2290e+02,  7.3257e+00,  2.2210e+02,  1.6011e+01,  1.8786e+02,\n",
       "          7.2530e-01, -2.2128e+00, -1.3680e-01, -7.9720e-01,  2.2012e+02,\n",
       "          1.0000e+00,  2.2177e+02,  9.1790e-01, -1.0030e-01,  1.0182e+00,\n",
       "          1.1782e+00, -6.9270e-01,  1.8709e+00,  2.1285e+02,  2.0744e+02,\n",
       "          6.0638e+01,  2.2174e+02,  2.1896e+02,  2.1512e+01,  1.7838e+01,\n",
       "          2.2500e+00,  1.8105e+00,  3.1431e+09,  2.4913e+01,  2.0658e+01,\n",
       "          5.3370e-01,  9.9230e-01,  1.0099e+00,  5.9836e+01,  2.1124e+02,\n",
       "          2.2059e+02,  8.7288e+01,  7.4214e+01,  9.8478e+01,  8.7288e+01,\n",
       "          1.0000e+02,  8.8521e+01,  2.2033e+02,  2.2409e+02,  5.0400e+00,\n",
       "          2.1972e+02, -1.8200e-02,  6.2756e+01, -7.8430e-01,  2.2168e+02,\n",
       "          2.2523e+02,  2.2916e+02,  2.2412e+02,  2.2900e+02,  2.2900e+02,\n",
       "          6.4215e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.1521e+08,  5.2816e+06,  1.1538e+01,  1.2955e+01,  1.7529e+00,\n",
       "          3.5000e+01,  1.0000e+02,  6.5000e+01,  4.0997e+00,  2.2816e+02,\n",
       "          2.2082e+02,  2.1348e+02, -4.1610e-01,  1.6955e+02,  1.5623e+01,\n",
       "          2.2384e+02,  9.5274e+00,  2.2263e+02,  1.5804e+01,  1.8565e+02,\n",
       "          1.1500e-01, -3.1600e-02, -9.8400e-02, -7.7330e-01,  2.2072e+02,\n",
       "          1.0000e+00,  2.2187e+02,  1.2352e+00,  1.6680e-01,  1.0685e+00,\n",
       "          1.7529e+00, -2.6890e-01,  2.0218e+00,  2.1300e+02,  2.0747e+02,\n",
       "          6.3817e+01,  2.2174e+02,  2.1940e+02,  2.0668e+01,  1.6946e+01,\n",
       "          4.4900e+00,  1.8008e+00,  3.1378e+09,  2.5021e+01,  2.0515e+01,\n",
       "          7.9300e-01,  2.0119e+00,  1.0201e+00,  5.7812e+01,  2.1195e+02,\n",
       "          2.2082e+02,  8.9005e+01,  8.4934e+01,  7.3823e+01,  8.9005e+01,\n",
       "          7.3734e+01,  9.1245e+01,  2.2035e+02,  2.2528e+02,  3.2200e+00,\n",
       "          2.2017e+02, -1.1000e-02,  5.9474e+01, -1.1226e+01,  2.2236e+02,\n",
       "          2.2900e+02,  2.3005e+02,  2.2683e+02,  2.2766e+02,  2.2766e+02,\n",
       "          5.2547e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.1852e+08,  5.7061e+06,  1.1395e+01,  1.2532e+01,  2.2713e+00,\n",
       "          3.0000e+01,  9.5000e+01,  6.5000e+01,  3.9907e+00,  2.2912e+02,\n",
       "          2.2112e+02,  2.1312e+02,  7.2400e-01,  1.4129e+02,  1.6752e+01,\n",
       "          2.2473e+02,  8.6724e+00,  2.2315e+02,  1.5740e+01,  1.8300e+02,\n",
       "          7.1060e-01,  2.0839e+00, -5.2400e-02, -7.4320e-01,  2.2138e+02,\n",
       "          1.0000e+00,  2.2202e+02,  1.5104e+00,  4.3550e-01,  1.0749e+00,\n",
       "          2.2713e+00,  1.8880e-01,  2.0825e+00,  2.1315e+02,  2.0750e+02,\n",
       "          6.3506e+01,  2.2174e+02,  2.1940e+02,  2.0521e+01,  1.6379e+01,\n",
       "          5.9800e+00,  1.7490e+00,  3.1426e+09,  2.4419e+01,  1.9489e+01,\n",
       "          1.0260e+00,  2.6914e+00,  1.0269e+00,  5.8376e+01,  2.1286e+02,\n",
       "          2.2112e+02,  8.3678e+01,  8.6657e+01,  7.8733e+01,  8.3678e+01,\n",
       "          8.1054e+01,  8.4929e+01,  2.2044e+02,  2.2636e+02,  1.9200e+00,\n",
       "          2.2071e+02, -2.9000e-03,  6.1556e+01, -8.8304e+00,  2.2306e+02,\n",
       "          2.2678e+02,  2.2847e+02,  2.2655e+02,  2.2817e+02,  2.2817e+02,\n",
       "          4.8138e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.1971e+08,  5.7402e+06,  1.0976e+01,  1.2262e+01,  2.3146e+00,\n",
       "          2.5000e+01,  9.0000e+01,  6.5000e+01,  3.9657e+00,  2.2967e+02,\n",
       "          2.2142e+02,  2.1316e+02, -8.2900e-02,  9.0268e+01,  1.0124e+01,\n",
       "          2.2509e+02,  3.0188e+00,  2.2342e+02,  1.5833e+01,  1.8229e+02,\n",
       "          1.7842e+00,  2.4230e+00, -4.0000e-02, -7.3480e-01,  2.2205e+02,\n",
       "          1.0000e+00,  2.2211e+02,  1.5350e+00,  6.5540e-01,  8.7960e-01,\n",
       "          2.3146e+00,  6.2830e-01,  1.6862e+00,  2.1328e+02,  2.0753e+02,\n",
       "          6.3796e+01,  2.2174e+02,  2.1940e+02,  2.1976e+01,  1.7430e+01,\n",
       "          5.9800e+00,  1.7548e+00,  3.1383e+09,  2.3344e+01,  1.8515e+01,\n",
       "          1.0442e+00,  2.7181e+00,  1.0272e+00,  5.5062e+01,  2.1372e+02,\n",
       "          2.2142e+02,  6.4165e+01,  7.8949e+01,  3.9941e+01,  6.4165e+01,\n",
       "          0.0000e+00,  5.1596e+01,  2.2061e+02,  2.2665e+02,  3.4900e+00,\n",
       "          2.2129e+02,  5.0000e-03,  6.1958e+01, -1.9070e+01,  2.2352e+02,\n",
       "          2.2616e+02,  2.2673e+02,  2.2468e+02,  2.2599e+02,  2.2599e+02,\n",
       "          4.2852e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.2444e+08,  6.7351e+06,  1.0524e+01,  1.2156e+01,  2.5636e+00,\n",
       "          2.0000e+01,  8.5000e+01,  6.5000e+01,  3.8754e+00,  2.3009e+02,\n",
       "          2.2193e+02,  2.1377e+02,  4.0280e-01,  8.1215e+01,  1.1195e+01,\n",
       "          2.2548e+02,  1.9441e+00,  2.2371e+02,  1.6130e+01,  1.8444e+02,\n",
       "          2.5614e+00,  3.5293e+00, -7.7500e-02, -7.5980e-01,  2.2265e+02,\n",
       "          1.0000e+00,  2.2234e+02,  1.5718e+00,  8.3870e-01,  7.3310e-01,\n",
       "          2.5636e+00,  1.0617e+00,  1.5019e+00,  2.1341e+02,  2.0756e+02,\n",
       "          6.4413e+01,  2.2174e+02,  2.1940e+02,  2.1828e+01,  1.6918e+01,\n",
       "          1.0290e+01,  1.7115e+00,  3.1433e+09,  2.2694e+01,  1.7589e+01,\n",
       "          1.1550e+00,  4.7608e+00,  1.0476e+00,  5.5597e+01,  2.1453e+02,\n",
       "          2.2193e+02,  5.2543e+01,  6.6795e+01,  3.8954e+01,  5.2543e+01,\n",
       "          1.1217e+01,  3.0757e+01,  2.2083e+02,  2.2698e+02,  2.1600e+00,\n",
       "          2.2190e+02,  1.2500e-02,  6.2705e+01, -1.7003e+01,  2.2400e+02,\n",
       "          2.2556e+02,  2.2648e+02,  2.2432e+02,  2.2643e+02,  2.2643e+02,\n",
       "          4.9524e+06,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_inputs[len(seq_inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(\"market_data/merged_data/\") if f.endswith('.csv')]\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f\"{filenames[i]}: {model(inputs[idx]).item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
