{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Normalizes all columns in a pandas DataFrame  using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The normalized DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = [col for col in df.columns]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_up_column(df):\n",
    "    # Create empty 'up' and 'down' columns\n",
    "    df['up'] = np.nan\n",
    "    \n",
    "    # Loop over the rows (skipping the first row)\n",
    "    for i in range(0, len(df)-1):\n",
    "        if df.loc[i+1, '4. close'] > df.loc[i, '4. close']:\n",
    "            df.loc[i, 'up'] = 1\n",
    "        else:\n",
    "            df.loc[i, 'up'] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APO</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>AROONOSC</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Real Upper Band</th>\n",
       "      <th>...</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. adjusted close</th>\n",
       "      <th>6. volume</th>\n",
       "      <th>7. dividend amount</th>\n",
       "      <th>8. split coefficient</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>company</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7819</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672490</td>\n",
       "      <td>0.258971</td>\n",
       "      <td>0.316359</td>\n",
       "      <td>0.754960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.302798</td>\n",
       "      <td>0.951020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929540</td>\n",
       "      <td>0.951991</td>\n",
       "      <td>0.944602</td>\n",
       "      <td>0.965739</td>\n",
       "      <td>0.140781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.766338</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7820</th>\n",
       "      <td>0.998778</td>\n",
       "      <td>0.664732</td>\n",
       "      <td>0.266415</td>\n",
       "      <td>0.306323</td>\n",
       "      <td>0.732013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.320301</td>\n",
       "      <td>0.943965</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764780</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7821</th>\n",
       "      <td>0.966297</td>\n",
       "      <td>0.581039</td>\n",
       "      <td>0.254699</td>\n",
       "      <td>0.296031</td>\n",
       "      <td>0.687362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.365638</td>\n",
       "      <td>0.918791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936562</td>\n",
       "      <td>0.941350</td>\n",
       "      <td>0.893466</td>\n",
       "      <td>0.916730</td>\n",
       "      <td>0.157345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755167</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7822</th>\n",
       "      <td>0.931217</td>\n",
       "      <td>0.468669</td>\n",
       "      <td>0.246950</td>\n",
       "      <td>0.288036</td>\n",
       "      <td>0.623677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.462123</td>\n",
       "      <td>0.939905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855206</td>\n",
       "      <td>0.814194</td>\n",
       "      <td>0.785275</td>\n",
       "      <td>0.813039</td>\n",
       "      <td>0.264651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502183</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7823</th>\n",
       "      <td>0.901446</td>\n",
       "      <td>0.365450</td>\n",
       "      <td>0.247488</td>\n",
       "      <td>0.285341</td>\n",
       "      <td>0.568705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.444907</td>\n",
       "      <td>0.964661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755206</td>\n",
       "      <td>0.776368</td>\n",
       "      <td>0.733902</td>\n",
       "      <td>0.763803</td>\n",
       "      <td>0.139751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415032</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Chaikin A/D     ADOSC       ADX      ADXR       APO  Aroon Down  \\\n",
       "7819     1.000000  0.672490  0.258971  0.316359  0.754960         0.0   \n",
       "7820     0.998778  0.664732  0.266415  0.306323  0.732013         0.0   \n",
       "7821     0.966297  0.581039  0.254699  0.296031  0.687362         0.0   \n",
       "7822     0.931217  0.468669  0.246950  0.288036  0.623677         1.0   \n",
       "7823     0.901446  0.365450  0.247488  0.285341  0.568705         1.0   \n",
       "\n",
       "      Aroon Up  AROONOSC       ATR  Real Upper Band  ...   2. high    3. low  \\\n",
       "7819      0.85     0.925  0.302798         0.951020  ...  0.929540  0.951991   \n",
       "7820      1.00     1.000  0.320301         0.943965  ...  1.000000  1.000000   \n",
       "7821      0.95     0.975  0.365638         0.918791  ...  0.936562  0.941350   \n",
       "7822      0.90     0.450  0.462123         0.939905  ...  0.855206  0.814194   \n",
       "7823      0.85     0.425  0.444907         0.964661  ...  0.755206  0.776368   \n",
       "\n",
       "      4. close  5. adjusted close  6. volume  7. dividend amount  \\\n",
       "7819  0.944602           0.965739   0.140781                 0.0   \n",
       "7820  0.980350           1.000000   0.265542                 0.0   \n",
       "7821  0.893466           0.916730   0.157345                 0.0   \n",
       "7822  0.785275           0.813039   0.264651                 0.0   \n",
       "7823  0.733902           0.763803   0.139751                 0.0   \n",
       "\n",
       "      8. split coefficient  sentiment  company   up  \n",
       "7819                   0.0   0.766338       28  1.0  \n",
       "7820                   0.0   0.764780       28  0.0  \n",
       "7821                   0.0   0.755167       28  0.0  \n",
       "7822                   0.0   0.502183       28  0.0  \n",
       "7823                   0.0   0.415032       28  NaN  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_csvs_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder into a single pandas DataFrame also normalizes before combining them.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the concatenated data from all CSV files in the input folder.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension to read all CSV files in the folder into a list of DataFrames.\n",
    "    dfs = [pd.read_csv(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # Use a list comprehension to get the filenames of all CSV files in the folder.\n",
    "    filenames = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    processed_dfs = []\n",
    "    i = 0\n",
    "    for df, filename in zip(dfs, filenames):\n",
    "        # Dont need the date column\n",
    "        df = df.drop(['date'], axis=1)\n",
    "        # normalize the dataframes before combining them\n",
    "        df = normalize_dataframe(df)\n",
    "        # for the neural network to understand the company name we need to convert it to a number\n",
    "        df['company'] = i\n",
    "        i += 1\n",
    "        df = add_up_column(df)\n",
    "        processed_dfs.append(df)\n",
    "    combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "df_full = combine_csvs_from_folder('market_data/merged_sentiment_data')\n",
    "\n",
    "df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[295, 560, 775, 1069, 1364, 1652, 1947, 2127, 2363, 2658, 2926, 3222, 3518, 3812, 4108, 4332, 4628, 4922, 5062, 5352, 5576, 5843, 6137, 6427, 6648, 6944, 7231, 7527, 7823]\n"
     ]
    }
   ],
   "source": [
    "#we need this for later\n",
    "def find_indices_of_test_rows(df):\n",
    "    indices = []\n",
    "    for i in range(1, len(df)):\n",
    "        if np.isnan(df.loc[i, 'up']):\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "idxs = find_indices_of_test_rows(df_full)\n",
    "print(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should one hot encode the company column\n",
    "# first we need to change it to a string so we can one hot encode it\n",
    "df_full['company'] = df_full['company'].astype(str)\n",
    "df_full = pd.get_dummies(df_full, columns=['company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nan rows\n",
    "df_train = df_full.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1353, 0.6197, 0.3955, 0.4593, 0.4225, 0.8500, 0.3500, 0.2500, 0.6032,\n",
      "        0.8557, 0.9394, 0.9225, 0.3350, 0.3643, 0.2875, 0.7970, 0.5260, 0.9030,\n",
      "        0.1048, 0.1016, 0.2951, 0.3613, 0.4410, 0.8099, 0.9726, 1.0000, 0.9390,\n",
      "        0.3662, 0.4006, 0.3425, 0.4225, 0.5731, 0.2418, 0.6032, 0.0000, 0.1638,\n",
      "        0.9062, 0.8188, 0.8024, 0.6250, 0.2994, 0.4558, 0.5325, 0.2254, 0.2272,\n",
      "        0.4146, 0.2649, 0.2648, 0.2875, 0.8989, 0.9394, 0.7918, 0.6421, 0.7659,\n",
      "        0.7918, 0.6817, 0.8398, 0.9730, 0.7253, 0.2869, 0.9107, 0.4255, 0.5867,\n",
      "        0.4532, 0.8684, 0.7363, 0.7490, 0.7196, 0.7078, 0.7047, 0.3275, 0.0000,\n",
      "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000])\n",
      "torch.Size([7795, 103])\n",
      "torch.Size([7795, 1])\n"
     ]
    }
   ],
   "source": [
    "# neural networks require tensors, so we need to convert our dataframes to tensors\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    inputs_columns = df.columns[df.columns != 'up']\n",
    "    inputs = torch.from_numpy(df.loc[:, inputs_columns].values.astype('float32'))\n",
    "    targets = torch.from_numpy(df.loc[:, ['up']].values.astype('float32'))\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "inputs, targets = df_to_tensor(df_train)\n",
    "print(inputs[0])\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch uses dataloaders to load data in batches\n",
    "\n",
    "batch_size = 2**8\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use gpu if avaliable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=103, out_features=1028, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=1028, out_features=512, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 1028),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1028, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# input size is 84 because we have 84 columns in our dataframe\n",
    "# output size is 1 because we are predicting up=1 or down=0\n",
    "input_size = inputs.shape[1]\n",
    "output_size = 1\n",
    "model = NN(input_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg_loss: 0.6940566897392273\n",
      "epoch: 100, avg_loss: 0.6851891875267029\n",
      "epoch: 200, avg_loss: 0.6410969793796539\n",
      "epoch: 300, avg_loss: 0.5571269541978836\n",
      "epoch: 400, avg_loss: 0.4104546010494232\n",
      "epoch: 500, avg_loss: 0.2672240138053894\n",
      "epoch: 600, avg_loss: 0.16646809503436089\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "training_losses = []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    #average the loss over all batches\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    training_losses.append(avg_loss)\n",
    "    if(epoch % 100 == 0 or epoch == 1):\n",
    "        print(f'epoch: {epoch}, avg_loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # Move data to GPU if available\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.float()) # BCE loss expects float inputs\n",
    "            val_loss += loss.item() * inputs.size(0) # Track total validation loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = torch.round(outputs)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return val_loss, accuracy\n",
    "\n",
    "val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "print(f'val_loss: {val_loss}, val_acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
